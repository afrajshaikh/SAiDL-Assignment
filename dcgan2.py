# -*- coding: utf-8 -*-
"""dcgan2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11tsJKjm3On3LY7cmYgRNDBRjubEZcO30
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import torch
import torch.nn as nn 
import matplotlib.pyplot as plt
import torch.utils.data
import torchvision
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import torch.autograd as Variable
import numpy as np
import torchvision.utils as vutils
import matplotlib.animation as animation
from IPython.display import HTML
import  torch.nn.parallel
import matplotlib

batch_size = 64
learning_rate = 0.0002
image_size = 64
channels = 3
noise_index = 100
num_epochs = 25
features_d = 64
features_g = 64
ngpu = 1
beta1 = 0.5


# Importing dataset.
train_dataset = dsets.CIFAR10(root='./data' , train=True,transform=transforms.Compose([
                               transforms.Resize(image_size),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),  download=True)
test_dataset = dsets.CIFAR10(root='./data' , train=False,transform=transforms.Compose([
                               transforms.Resize(image_size),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]), download =True)


# Load dataset.
train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle = True,num_workers=2)

test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle = False,num_workers=2)

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

class Generator(nn.Module):
  def __init__(self, ngpu):
    super(Generator, self).__init__()
    self.net = nn.Sequential(
        #Linear as input.
        nn.ConvTranspose2d(noise_index, features_g*8, kernel_size=4, stride=1, padding=0, bias=False),
        nn.BatchNorm2d(features_g*8),
        nn.ReLU(),

        #4X4
        nn.ConvTranspose2d(features_g*8, features_g*4, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(features_g*4),
        nn.ReLU(),

        #8X8
        nn.ConvTranspose2d(features_g*4, features_g*2, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(features_g*2),
        nn.ReLU(),

        #16X16
        nn.ConvTranspose2d(features_g*2, features_g, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(features_g),
        nn.ReLU(),

        #32X32
        nn.ConvTranspose2d(features_g, channels, kernel_size=4, stride=2, padding=1, bias=False),
        nn.Tanh()

        #64X64 as output.

    )

  def forward(self,x):
    return self.net(x)

netG = Generator(ngpu).to(device)

if (device.type == 'cuda') and (ngpu>1):
  netG = nn.DataParallel(netG, list(range(ngpu)))

netG.apply(weights_init)

print(netG)

class Discriminator(nn.Module):
  def __init__(self,ngpu):
    super(Discriminator,self).__init__()
    self.net = nn.Sequential(
        
        #3X64X64 as input.
        nn.Conv2d(channels, features_d, kernel_size=4, stride=2, padding=1, bias=False),
        nn.LeakyReLU(0.2),

        #features_dX32X32
        nn.Conv2d(features_d, features_d*2, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(features_d*2),
        nn.LeakyReLU(0.2),

        #features_d*2X16X16
        nn.Conv2d(features_d*2, features_d*4, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(features_d*4),
        nn.LeakyReLU(0.2),

        #features_d*4X8X8
        nn.Conv2d(features_d*4, features_d*8, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(features_d*8),
        nn.LeakyReLU(0.2),

        #features_dX4X4
        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=1, padding=0, bias=False),
        nn.Sigmoid()

    )
  def forward(self, x):
    return self.net(x)





netD = Discriminator(ngpu).to(device)

if (device.type == 'cuda') and (ngpu > 1):
  netD = nn.DataParallel(netD, list(range(ngpu)))

netD.apply(weights_init)

print(netD)

criterion = nn.BCELoss()

fixed_noise = torch.rand(64, noise_index, 1, 1, device = device)


real_label = 0.9
fake_label = 0.1


# Defining Optimizers.
optimizerD = torch.optim.Adam(netD.parameters(), lr = learning_rate, betas = (beta1,0.999))
optimizerG = torch.optim.Adam(netG.parameters(), lr = learning_rate, betas = (beta1, 0.999))


img_list = []
G_losses = []
D_losses = []
iters = 0


print("Training Started...")

for epoch in range(num_epochs):


  for i, data in enumerate(train_loader, 0):

    # Update D network i.e Maximise log(D(x)) + log(1-D(G(z)))


    netD.zero_grad()

    real_cpu = data[0].to(device)
    b_size = real_cpu.size(0)
    label = torch.full((b_size,), real_label, device = device)

    output = netD(real_cpu).view(-1)

    errD_real = criterion(output, label)

    errD_real.backward()
    D_x = output.mean().item()

    #Train with fake batch


    noise = torch.randn(b_size, noise_index, 1, 1, device = device)

    fake = netG(noise)
    label.fill_(fake_label)

    output = netD(fake.detach()).view(-1)

    errD_fake = criterion(output, label)
    errD_fake.backward()
    D_G_z1 = output.mean().item()
    errD = errD_real + errD_fake

    optimizerD.step()


    # Update G i.e maximise log(D(G(z)))


    netG.zero_grad()
    label.fill_(real_label)
    output = netD(fake).view(-1)
    errG = criterion(output, label)
    errG.backward()
    D_G_z2 = output.mean().item()

    optimizerG.step()

    if i % 50 == 0:
      print('[%d/%d] [%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f' % (epoch, num_epochs, i, len(train_loader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))
    

    G_losses.append(errG.item())
    D_losses.append(errD.item())

    if iters % 500 ==0 or ((epoch == num_epochs-1) and (i == len(train_loader) -1 )):
      with torch.no_grad():
        fake = netG(fixed_noise).detach().cpu()
      
      img_list.append(vutils.make_grid(fake, padding =2, normalize = True))
    iters+=1

plt.figure(figsize = (10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses, label = "G")
plt.plot(D_losses, label = "D")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

fig = plt.figure(figsize = (8,8))
plt.axis("off")
matplotlib.rcParams['animation.embed_limit'] = 2**128
ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated = True)] for i in img_list]
ani = animation.ArtistAnimation(fig, ims, interval = 1000, repeat_delay = 1000, blit = True)

HTML(ani.to_jshtml())

real_batch = next(iter(train_loader))

plt.figure(figsize=(15,15))
plt.subplot(1, 2, 1)
plt.axis("off")
plt.title("Real Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(), (1,2,0)))
plt.subplot(1,2,2)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(np.transpose(img_list[-1], (1,2,0)))
plt.show()